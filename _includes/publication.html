<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 400px;
			height: 230px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 50px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 17.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
			    margin-top: 10px;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 700px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			padding-bottom: 0px;
			min-height: 150px;

		}
		.paperTitle{
			font-size:14.0pt;
			mso-bidi-font-size:18.0pt;
			font-family:Times;
			mso-bidi-font-family:Times;
			margin-top: 10px;
			margin-bottom: 10px;
			font-weight: bold;
		}
		.paperName,.paperPub{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    line-height:150%;
		}
		.link{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 10px;
		    margin-bottom: 0px;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height: 300px;
			width: 950px;

		}
		.short div.sub-left, .short div.sub-right{
			height:160px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
</head>

<h3>
	<a name='pre'></a> Selected Preprint
</h3>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/FishFormer.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>FishFormer: Annulus Slicing-based Transformer for Fisheye Rectification with Efficacy Domain Exploration</strong><br />
					   <strong>Shangrong Yang</strong>, Chunyu Lin, Shujuan Huang, Kang Liao, Yao Zhao<br />
					   <a href="https://arxiv.org/pdf/2207.01925.pdf">[arXiv]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

<h3>
	<a name='publications'></a> Selected Publications
</h3>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/Dual-Diffusion.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization</strong><br />
					   <strong>Shangrong Yang</strong>, Chunyu Lin, Kang Liao, Yao Zhao<br />
					   International Conference on Computer Vision (<strong>ICCV 2023</strong>)<br />
					   <a href="https://arxiv.org/pdf/2301.11785.pdf">[arXiv]</a>
					   <a href="">[Github]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

                 <div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/Dual-Diffusion.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Spatiotemporal Deformation Perception for Fisheye Video Rectification</strong><br />
					   <strong>Shangrong Yang</strong>, Chunyu Lin, Kang Liao, Yao Zhao<br />
					   International Conference on Computer Vision (<strong>AAAI 2023</strong>)<br />
					   <a href="https://arxiv.org/pdf/2302.03934.pdf">[arXiv]</a>
					   <a href="">[Github]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/ICCV21-ROP.png" width="240" height="130">					
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">					
					  <strong>Towards Complete Scene and Regular Shape for Distortion Rectification by Curve-Aware Extrapolation</strong><br />
					  Kang Liao, Chunyu Lin, Yunchao Wei, Feng Li, <strong>Shangrong Yang</strong>, Yao Zhao<br />
					   International Conference on Computer Vision (<strong>ICCV 2021</strong>)<br />
					   <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liao_Towards_Complete_Scene_and_Regular_Shape_for_Distortion_Rectification_by_ICCV_2021_paper.pdf">[PDF]</a>
					</p>
				  </div>
				</div>
				</div>
			</div>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/CVPR21-PCN.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Appearance Flow-based Progressively Complementary Network for Distortion Rectification</strong><br />
					  <strong>Shangrong Yang*</strong>strong>, Chunyu Lin*, Kang Liao*, Chunjie Zhang, Yao Zhao<br />
					   IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR 2021</strong>)<br />
					   (*Equal contribution.)<br />
					   <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Progressively_Complementary_Network_for_Fisheye_Image_Rectification_Using_Appearance_Flow_CVPR_2021_paper.pdf">[PDF]</a>
					   <a href="https://github.com/uof1745-cmd/PCN">[Github]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>



		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/TIP21-unsupervised.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Unsupervised Deep Image Stitching: Reconstructing Stitched Features to Images</strong><br />
					  Lang Nie, Chunyu Lin, <strong>Kang Liao</strong>, Shuaicheng Liu, Yao Zhao<br />
					   IEEE Transactions on Image Processing (<strong>IEEE TIP 2021</strong>)<br />
					   <a href="https://ieeexplore.ieee.org/document/9472883">[PDF]</a>
					   <a href="https://github.com/nie-lang/UnsupervisedDeepImageStitching">[Github]</a>
					   <a href="https://zhuanlan.zhihu.com/p/386863945">[Chinese Blog]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>


